{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be trying to understand sentiment of tweets about the company Apple. By using the twitter data we can hope to understand the public perception a bit better.\n",
    "\n",
    "Our challenge is to see if we can correctly classify tweets as being either positive or negative.\n",
    "\n",
    "Problem Statement:\n",
    "•\tCorrectly classify the tweets as being positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using: nltk.NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:15:50.209160Z",
     "start_time": "2020-09-17T13:15:50.204146Z"
    }
   },
   "outputs": [],
   "source": [
    "## Importing the necessary libraries along with the standard import\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re # this is the regular expression library which helps us manipulate text (strings) fairly easily and intuitively\n",
    "import nltk # this is the Natural Language Tool Kit which contains a lot of functionalities for text analytics\n",
    "import matplotlib.pyplot as plt\n",
    "import string # this is used for string manipulations\n",
    "import matplotlib\n",
    "import warnings \n",
    "warnings.filterwarnings( \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:15:51.798506Z",
     "start_time": "2020-09-17T13:15:51.303200Z"
    }
   },
   "outputs": [],
   "source": [
    "## Let us check the version of the various libraries\n",
    "print('Numpy version:',np.__version__)\n",
    "print('Pandas version:',pd.__version__)\n",
    "print('Regular Expression version:',re.__version__)\n",
    "print('Natural Language Tool Kit version:',nltk.__version__)\n",
    "print('Matplotlib version:',matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier.\n",
    "Now, let us load the data and look at a few other text mining functionalities that Python offers us and then go on to fit a classifier algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:17:23.843541Z",
     "start_time": "2020-09-17T13:17:23.836513Z"
    }
   },
   "outputs": [],
   "source": [
    "## Loading the dataset\n",
    "Apple_tweets = pd.read_csv(\"Apple_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apple_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Exploration in Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To create a temporary function lambda can be used. These functions do not require a name like a def function, however the output is same as defining a permanent function**\n",
    "**As these function are temporary, memory comsumption is less in comparison to permanent function. Also there are multiple ways to get a similar output**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:17:54.915260Z",
     "start_time": "2020-09-17T13:17:54.880323Z"
    }
   },
   "outputs": [],
   "source": [
    "## Let's get a word count without writing a lambda function\n",
    "\n",
    "Apple_tweets['totalwords'] = [len(x.split()) for x in Apple_tweets['Tweet']]\n",
    "Apple_tweets[['Tweet','totalwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:24:09.462643Z",
     "start_time": "2020-09-17T13:24:09.449644Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['word_count'] = Apple_tweets['Tweet'].apply(lambda x: len(str(x).split(\" \")))\n",
    "Apple_tweets[['Tweet','word_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Characters- including spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:26:56.611074Z",
     "start_time": "2020-09-17T13:26:56.597112Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Apple_tweets['char_count'] = Apple_tweets['Tweet'].str.len()\n",
    "Apple_tweets[['Tweet','char_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:45:51.927637Z",
     "start_time": "2020-09-17T13:45:51.910685Z"
    }
   },
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split() #splitting the words separately from the input taken\n",
    "    return (sum(len(word) for word in words)/len(words)) # getting the average number of words in the each of the entries\n",
    "\n",
    "Apple_tweets['avg_word'] = Apple_tweets['Tweet'].apply(lambda x: avg_word(x))\n",
    "Apple_tweets[['Tweet','avg_word']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:46:06.469069Z",
     "start_time": "2020-09-17T13:46:06.418170Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "Apple_tweets['stopwords'] = Apple_tweets['Tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "Apple_tweets[['Tweet','stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of special character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:46:07.922780Z",
     "start_time": "2020-09-17T13:46:07.904830Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['hastags'] = Apple_tweets['Tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "Apple_tweets[['Tweet','hastags']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:47:02.420988Z",
     "start_time": "2020-09-17T13:47:02.407991Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['numerics'] = Apple_tweets['Tweet'].apply(lambda x: len(re.findall('[0-9]',x)))\n",
    "Apple_tweets[['Tweet','numerics']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Uppercase Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:48:14.374796Z",
     "start_time": "2020-09-17T13:48:14.360836Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['upper'] = Apple_tweets['Tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "Apple_tweets[['Tweet','upper']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Uppercase Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:48:38.429737Z",
     "start_time": "2020-09-17T13:48:38.412749Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Apple_tweets['upper_letter'] = Apple_tweets['Tweet'].apply(lambda x: len(re.findall('[A-Z]',x)))\n",
    "Apple_tweets[['Tweet','upper_letter']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Apple_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower Case conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:48:40.866892Z",
     "start_time": "2020-09-17T13:48:40.853895Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['Tweet'] = Apple_tweets['Tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "Apple_tweets['Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:48:41.947072Z",
     "start_time": "2020-09-17T13:48:41.936063Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Apple_tweets['Tweet'] = Apple_tweets['Tweet'].str.replace('[^\\w\\s]','')\n",
    "#\\w: Returns a match where the string contains any word characters (characters from a to Z, digits from 0-9, and the underscore _ character)\n",
    "#\\s: Returns a match where the string contains a white space character.\n",
    "#[^]: Returns a match for any character EXCEPT what is written after it.\n",
    "Apple_tweets['Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:48:43.018210Z",
     "start_time": "2020-09-17T13:48:42.973290Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "Apple_tweets['Tweet'] = Apple_tweets['Tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "Apple_tweets['Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Words Removal\n",
    "1. **We will create a list of 10 frequently occuring words and then decide if we need to remove it or retain it.**\n",
    "2. **Reason is that this file has tweets related to Apple.. So no point in keeping the word like Apple, unless we have tweets from other brands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:48:44.047416Z",
     "start_time": "2020-09-17T13:48:44.034451Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(Apple_tweets['Tweet']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:48:45.188398Z",
     "start_time": "2020-09-17T13:48:45.184409Z"
    }
   },
   "outputs": [],
   "source": [
    "freq =['apple','get']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **As we are talking about multiple products hence iphone will be kept, similarly some tweets do relate to old products without mentioning the word old, hence even new would be kept in the tweets.**\n",
    "2. **hence only apple and get would be removed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:48:46.194713Z",
     "start_time": "2020-09-17T13:48:46.184700Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets['Tweet'] = Apple_tweets['Tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "Apple_tweets['Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare Words Removal\n",
    "**This is done as association of these less occurring words with the existing words could be a noise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:48:47.191921Z",
     "start_time": "2020-09-17T13:48:47.178880Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(Apple_tweets['Tweet']).split()).value_counts()[-10:]\n",
    "freq\n",
    "## As it is difficult to make out if these words will have association in text analytics or not, \n",
    "## hence to start with these words are kept in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming -refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:48:49.524668Z",
     "start_time": "2020-09-17T13:48:49.303254Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "Apple_tweets['Tweet'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:48:50.527947Z",
     "start_time": "2020-09-17T13:48:50.523957Z"
    }
   },
   "outputs": [],
   "source": [
    "def Tweet(x):\n",
    "    if x >= 0:\n",
    "        return \"Positive\"\n",
    "    else: return \"Negative\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T10:52:41.105760Z",
     "start_time": "2020-04-10T10:52:41.100002Z"
    }
   },
   "source": [
    "### Now to get the sentiments as positive and negative , convert the Avg column . If value is >= 0  then tweet is Positive, else tweet is Negative. This will make a dependent variable as a binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:48:51.525313Z",
     "start_time": "2020-09-17T13:48:51.511317Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets[\"Sentiment\"] = Apple_tweets[\"Avg\"].apply(Tweet)\n",
    "\n",
    "Apple_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:48:52.541303Z",
     "start_time": "2020-09-17T13:48:52.504660Z"
    }
   },
   "outputs": [],
   "source": [
    "Apple_tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at distribution of different sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:48:54.107314Z",
     "start_time": "2020-09-17T13:48:53.521680Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "Apple_tweets.Sentiment.value_counts().plot(kind='pie', autopct='%1.0f%%', colors=[\"green\",\"red\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:48:56.405729Z",
     "start_time": "2020-09-17T13:48:56.396722Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Apple_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apple_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:51:28.302207Z",
     "start_time": "2020-09-17T13:51:28.298218Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_features = Apple_tweets.iloc[:, 0].values\n",
    "labels = Apple_tweets.iloc[:, 10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:51:32.650071Z",
     "start_time": "2020-09-17T13:51:32.644088Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:51:33.713226Z",
     "start_time": "2020-09-17T13:51:33.708242Z"
    }
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer\n",
    "\n",
    "More here - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:51:41.323542Z",
     "start_time": "2020-09-17T13:51:41.298612Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8)\n",
    "processed_features = vectorizer.fit_transform(processed_features).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:51:42.364669Z",
     "start_time": "2020-09-17T13:51:42.358685Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:51:43.387966Z",
     "start_time": "2020-09-17T13:51:43.361003Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Extra Knowledge Bytes (TF-IDF)\n",
    "## TF= No of rep of words in a sentence/No of words in a sentence\n",
    "##IDF= Log(no of sentences/No sentences containing this particular word)\n",
    "# Let's see how our TD-IDF looks like (sorting by the feature named 5s)\n",
    "# Creating the TF-IDF with the feature names given by the TFIDF vectorizer, sorting it for unerstanding.\n",
    "# Let's chain the .head() method on the DataFrame to inspect the first few observations of the TD-IDF sorted by '5s'\n",
    "pd.DataFrame(processed_features, columns = vectorizer.get_feature_names()).sort_values(by = '5s', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:59:11.835866Z",
     "start_time": "2020-09-17T13:59:11.830848Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:59:12.937885Z",
     "start_time": "2020-09-17T13:59:12.930903Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:59:14.276109Z",
     "start_time": "2020-09-17T13:59:13.935220Z"
    }
   },
   "outputs": [],
   "source": [
    "# To model the Gaussian Navie Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:59:15.280249Z",
     "start_time": "2020-09-17T13:59:15.270278Z"
    }
   },
   "outputs": [],
   "source": [
    "NB_model = GaussianNB(var_smoothing=1e-15)\n",
    "NB_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:59:16.332061Z",
     "start_time": "2020-09-17T13:59:16.258633Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_predict = NB_model.predict(X_train)\n",
    "model_score = NB_model.score(X_train, y_train)                      ## Accuracy\n",
    "print(model_score)\n",
    "print(metrics.confusion_matrix(y_train, y_train_predict))          ## confusion_matrix\n",
    "print(metrics.classification_report(y_train, y_train_predict))     ## classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:59:18.039302Z",
     "start_time": "2020-09-17T13:59:18.012345Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Performance Matrix on test data set\n",
    "y_test_predict = NB_model.predict(X_test)\n",
    "model_score = NB_model.score(X_test, y_test)                    ## Accuracy\n",
    "print(model_score)\n",
    "print(metrics.confusion_matrix(y_test, y_test_predict))         ## confusion_matrix\n",
    "print(metrics.classification_report(y_test, y_test_predict))    ## classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data Probability Prediction\n",
    "pred_prob_train = NB_model.predict_proba(X_train)\n",
    "\n",
    "# Test Data Probability Prediction\n",
    "pred_prob_test = NB_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "probs = NB_model.predict_proba(X_train)\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = probs[:, 1]\n",
    "# calculate AUC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_train, probs)\n",
    "print('AUC: %.3f' % auc)\n",
    "# calculate roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train, probs,pos_label='Positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC and ROC for the training data\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC and ROC for the training data\n",
    "\n",
    "# calculate AUC\n",
    "auc = metrics.roc_auc_score(y_train,pred_prob_train[:,1])\n",
    "print('AUC for the Training Data: %.3f' % auc)\n",
    "\n",
    "#  calculate roc curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train,pred_prob_train[:,1],pos_label='Positive')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "plt.plot(fpr, tpr, marker='.',label = 'Training Data')\n",
    "\n",
    "\n",
    "# AUC and ROC for the test data\n",
    "\n",
    "# calculate AUC\n",
    "auc = metrics.roc_auc_score(y_test,pred_prob_test[:,1])\n",
    "print('AUC for the Test Data: %.3f' % auc)\n",
    "\n",
    "#  calculate roc curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test,pred_prob_test[:,1],pos_label='Positive')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "plt.plot(fpr, tpr, marker='.',label='Test Data')\n",
    "# show the plot\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pl. note - Model building is an iterative process. Model performance both on the test and train dataset can be improved using feature engineering, feature extraction, hyper parameter tuning (including combination of various parameters).** \n",
    "\n",
    "**Model has to match the business objective and hence various permutation and combinations can be tried on to refine the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:51:53.273873Z",
     "start_time": "2020-09-17T13:51:53.267852Z"
    }
   },
   "outputs": [],
   "source": [
    "# Recall Apple_Tweets['Tweet'] from above. It has been processed at a basic level!\n",
    "\n",
    "Apple_tweets['Tweet'].iloc[30:31] #Checking a tweet at random!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:51:54.343007Z",
     "start_time": "2020-09-17T13:51:54.338019Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing symbols and punctuations \n",
    "# further_clean = Apple_tweets['Tweet'].str.replace('[^\\w\\s]','')\n",
    "# stop_words.remove()  To remove words from the list of stop words\n",
    "# Extending the list of stop words (including words like Apple, bitly, dear, please, etc.)\n",
    "stop_words = list(stopwords.words('english'))\n",
    "stop_words.extend([\"apple\", \"http\",\"bit\",\"bitly\",\"bit ly\", \"dear\", \"im\", \"i'm\", \"please\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:51:55.693364Z",
     "start_time": "2020-09-17T13:51:55.344534Z"
    }
   },
   "outputs": [],
   "source": [
    "#Removing stop words (extended list as above) from the corpus \n",
    "\n",
    "corpus = Apple_tweets['Tweet'].apply(lambda x: ' '.join([z for z in x.split() if z not in stop_words])) \n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:51:56.694654Z",
     "start_time": "2020-09-17T13:51:56.691688Z"
    }
   },
   "outputs": [],
   "source": [
    "wc_a = ' '.join(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wc_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T13:52:15.853157Z",
     "start_time": "2020-09-17T13:51:57.709686Z"
    }
   },
   "outputs": [],
   "source": [
    "# Word Cloud \n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width = 3000, height = 3000, \n",
    "                background_color ='black', \n",
    "                min_font_size = 10, random_state=100).generate(wc_a) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (6, 6), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\")\n",
    "plt.xlabel('Word Cloud')\n",
    "plt.tight_layout(pad = 0) \n",
    "\n",
    "print(\"Word Cloud for Apple_Tweets (after cleaning)!!\")\n",
    "plt.show()\n",
    "\n",
    "#Tip: You can specify stopwords, regex (punctuations/symbols) in the wordcloud itself, check CTRL+TAB on the wordcloud fuction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
