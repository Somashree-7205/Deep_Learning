{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8570ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "%matplotlib inline\n",
    "import warnings \n",
    "warnings.filterwarnings( \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98920126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Train test split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db87b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pre-processing of text\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae97dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, GlobalAveragePooling1D, Flatten, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b7562",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('spam.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6a5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['v1','v2']]\n",
    "df.columns=['Category','Content']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85caa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all strings into a single string\n",
    "spam_text=[i for i in df['Content']]\n",
    "spam_text_final=''\n",
    "for i in spam_text:\n",
    "  spam_text_final=spam_text_final+str(i)\n",
    "spam_text_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3c7c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a word cloud\n",
    "ham_msg_cloud = WordCloud(width =520, height =260, stopwords = STOPWORDS, max_font_size = 50, background_color='green', colormap='magma').generate(spam_text_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261847e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "plt.imshow(ham_msg_cloud, interpolation ='bicubic')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc7dc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647c41a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4602f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de420faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand distribution of ham and spam messages\n",
    "\n",
    "sns.countplot(x='Category',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfadecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc732b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Text_Length'] = new_df['Content'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93976295",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_df['Content_Type'] = new_df['Category'].map({'ham':0,'spam':1})\n",
    "content_label = new_df['Content_Type'].values\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348077f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Content'] = new_df['Content'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163bed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b0579",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Content'] = new_df['Content'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "new_df['Content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6c644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df['Content'] = new_df['Content'].str.replace('[^\\w\\s]','')\n",
    "#\\w: Returns a match where the string contains any word characters (characters from a to Z, digits from 0-9, and the underscore _ character)\n",
    "#\\s: Returns a match where the string contains a white space character.\n",
    "#[^]: Returns a match for any character EXCEPT what is written after it.\n",
    "new_df['Content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords Removal\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "new_df['Content'] = new_df['Content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "new_df['Content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeecd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(new_df['Content']).split()).value_counts()[:20]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d6625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c561b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd91cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = WordNetLemmatizer()\n",
    "new_df['Content'].apply(lambda x: \" \".join([lt.lemmatize(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8dff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test vs train\n",
    "x_train, x_test, y_train, y_test = train_test_split(new_df['Content'], content_label, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90450448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "max_len=50    ## texts longer than 50 tokens will be truncated\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_token_1='<OOV>'# out of vocabulary token\n",
    "vocab_size=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [str(item) for item in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c2cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting text into numerical representation - create a tokenizer and fit it on the training data\n",
    "tokenizer = Tokenizer(num_words = vocab_size,    # maximum number of words to keep in the tokenizer's vocabulary\n",
    "                      char_level = False,\n",
    "                      oov_token = oov_token_1)\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bec1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f758573",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index=tokenizer.word_index\n",
    "total_words = len(word_index)\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3701c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the text data into numerical sequences & padding the sequence for same length\n",
    "training_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "training_padded = pad_sequences(training_sequences,\n",
    "                                maxlen = max_len,\n",
    "                                padding = padding_type,\n",
    "                                truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ca197",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "testing_padded = pad_sequences(testing_sequences,\n",
    "                               maxlen = max_len,\n",
    "                               padding = padding_type,\n",
    "                               truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6702aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building and Dense Model Architecture\n",
    "vocab_size=500\n",
    "embedding_dim = 12   # specifies the dimension of the word embeddings used in the embedding layer\n",
    "drop_value = 0.2\n",
    "##n_dense = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a1513",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_rnn = Sequential()\n",
    "Model_rnn.add(Embedding(vocab_size,\n",
    "                        embedding_dim,  # each word in the vocabulary will be represented by a dense vector of 12 dimensions.\n",
    "                        input_length = max_len))\n",
    "Model_rnn.add(GlobalAveragePooling1D())\n",
    "Model_rnn.add(Dense(24,activation='relu'))\n",
    "Model_rnn.add(Dropout(drop_value))\n",
    "Model_rnn.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a58da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_rnn.summary()\n",
    "Model_rnn.compile(loss = 'binary_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c12106",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fcfc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "history = Model_rnn.fit(training_padded,\n",
    "                        y_train,\n",
    "                        epochs=num_epochs,\n",
    "                        validation_data=(testing_padded,y_test),\n",
    "                        callbacks =[early_stop],\n",
    "                        verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972fa59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_rnn.evaluate(testing_padded,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_probs = Model_rnn.predict(training_padded)\n",
    "\n",
    "# Convert the probabilities to predicted labels (0 or 1) using a threshold (e.g., 0.5)\n",
    "threshold = 0.5\n",
    "y_pred = (y_probs >= threshold).astype(int)\n",
    "\n",
    "# Step 2: Generate the classification report\n",
    "target_names = ['Class 0 (Negative)', 'Class 1 (Positive)']  # Replace with actual class names if available\n",
    "report = classification_report(y_train, y_pred, target_names=target_names)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have trained the model (Model_rnn) and have the testing data and true labels (testing_padded and y_test)\n",
    "\n",
    "# Step 1: Make predictions on the test dataset using the trained model\n",
    "y_probs = Model_rnn.predict(testing_padded)\n",
    "\n",
    "# Convert the probabilities to predicted labels (0 or 1) using a threshold (e.g., 0.5)\n",
    "threshold = 0.5\n",
    "y_pred = (y_probs >= threshold).astype(int)\n",
    "\n",
    "# Step 2: Generate the classification report\n",
    "target_names = ['Class 0 (Negative)', 'Class 1 (Positive)']  # Replace with actual class names if available\n",
    "report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55502912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters and LSTM model\n",
    "n_lstm = 128\n",
    "drop_lstm = 0.2\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_size,embedding_dim,input_length=max_len))\n",
    "model_lstm.add(SpatialDropout1D(drop_lstm))\n",
    "model_lstm.add(LSTM(n_lstm,return_sequences=False))\n",
    "model_lstm.add(Dropout(drop_lstm))\n",
    "model_lstm.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cdd36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.compile(loss='binary_crossentropy',\n",
    "                   optimizer='adam',\n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc24d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2)\n",
    "history = model_lstm.fit(training_padded,\n",
    "                         y_train,\n",
    "                         epochs=num_epochs,\n",
    "                         validation_data=(testing_padded,y_test),\n",
    "                         callbacks =[early_stop],\n",
    "                         verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f9815",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.evaluate(testing_padded,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f525028",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dense_results = model_lstm.evaluate(training_padded, np.asarray(y_train), verbose=2, batch_size=256)\n",
    "valid_dense_results=model_lstm.evaluate(testing_padded, np.asarray(y_test), verbose=2, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e74c65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
